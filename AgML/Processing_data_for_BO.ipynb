{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as font_manager\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://github.com/google/fonts/raw/main/ofl/ibmplexmono/IBMPlexMono-Regular.ttf\",\n",
    "    \"IBMPlexMono-Regular.ttf\",\n",
    ")\n",
    "fe = font_manager.FontEntry(fname=\"IBMPlexMono-Regular.ttf\", name=\"plexmono\")\n",
    "font_manager.fontManager.ttflist.append(fe)\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"axes.facecolor\": \"#f5f4e9\",\n",
    "        \"grid.color\": \"#AAAAAA\",\n",
    "        \"axes.edgecolor\": \"#333333\",\n",
    "        \"figure.facecolor\": \"#FFFFFF\",\n",
    "        \"axes.grid\": False,\n",
    "        \"axes.prop_cycle\": plt.cycler(\"color\", plt.cm.Dark2.colors),\n",
    "        \"font.family\": fe.name,\n",
    "        \"figure.figsize\": (3.5, 3.5 / 1.2),\n",
    "        \"ytick.left\": True,\n",
    "        \"xtick.bottom\": True,\n",
    "    }\n",
    ")\n",
    "\n",
    "import random\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util functions to process Spectre information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "# TODO: Refactor all these functions to take into account that we also have abs spectra now\n",
    "\n",
    "def _get_peaks(x, y):\n",
    "    peaks = []\n",
    "    th = 0.001\n",
    "    for i in range(1, len(x)-1):\n",
    "        if y[i] > y[i-1]+th and y[i] > y[i+1]+th:\n",
    "            peaks.append((x[i], y[i]))\n",
    "    return peaks\n",
    "\n",
    "def get_peaks(x, y):\n",
    "    x = np.array(x)\n",
    "    p1_i = np.where(x == 450)[0][0]\n",
    "    p1_e = np.where(x == 550)[0][0]\n",
    "    p2_i = np.where(x == 550)[0][0]\n",
    "    p2_e = np.where(x == 800)[0][0]\n",
    "\n",
    "    x1 = x[p1_i:p1_e]\n",
    "    y1 = y[p1_i:p1_e]\n",
    "    x2 = x[p2_i:p2_e]\n",
    "    y2 = y[p2_i:p2_e]\n",
    "\n",
    "    return x1[np.argmax(y1)], x2[np.argmax(y2)]\n",
    "\n",
    "def get_area_under_peaks(x, y, norm=None):\n",
    "    if not norm:\n",
    "        return np.trapz(y, x)\n",
    "    \n",
    "    x = np.array(x)\n",
    "    p1_i = np.where(x == 450)[0][0]\n",
    "    p1_e = np.where(x == 550)[0][0]\n",
    "    p2_i = np.where(x == 550)[0][0]\n",
    "    p2_e = np.where(x == 800)[0][0]\n",
    "\n",
    "    x1 = x[p1_i:p1_e]\n",
    "    y1 = y[p1_i:p1_e]\n",
    "    x2 = x[p2_i:p2_e]\n",
    "    y2 = y[p2_i:p2_e]\n",
    "\n",
    "    return np.trapz(y1, x1)/norm, np.trapz(y2, x2)/norm\n",
    "\n",
    "def process_spectrum(x, y, norm_area):\n",
    "    peaks = get_peaks(x, y)\n",
    "    areas = get_area_under_peaks(x, y, norm_area)\n",
    "    return peaks, areas\n",
    "\n",
    "def smooth_spectrum(x, y, window=4):\n",
    "    y = np.array(y)\n",
    "    y_smooth = np.zeros_like(y)\n",
    "    for i in range(len(y)):\n",
    "        if i < window:\n",
    "            y_smooth[i] = np.mean(y[:i+window])\n",
    "        elif i > len(y) - window:\n",
    "            y_smooth[i] = np.mean(y[i-window:])\n",
    "        else:\n",
    "            y_smooth[i] = np.mean(y[i-window:i+window])\n",
    "    return x, y_smooth.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get f=AUC2*QY for new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of files in Data/fluoressence_data starting in M\n",
    "import os\n",
    "files = os.listdir('Data/fluorescence_data/proposed_trials/BO/')\n",
    "#files = os.listdir('Data/fluorescence_data/proposed_trials/NLP/')\n",
    "#split files in two lists, one .txt and one .csv\n",
    "txt_files = [file for file in files if file.endswith('.txt')]\n",
    "csv_files = [file for file in files if file.endswith('.csv')]\n",
    "#organize them together such that the first .txt file corresponds to the first .csv file\n",
    "txt_files.sort() #To make sure they are zipped in the right order correctly\n",
    "csv_files.sort() #To make sure they are zipped in the right order correctly\n",
    "\n",
    "for txt, csv in zip(txt_files, csv_files):\n",
    "    #with open(f\"Data/fluorescence_data/proposed_trials/NLP/{txt}\", 'r') as input:\n",
    "    with open(f\"Data/fluorescence_data/proposed_trials/BO/{txt}\", 'r') as input:\n",
    "                lines = input.readlines()\n",
    "                x = []\n",
    "                y = []\n",
    "                for line in lines:\n",
    "                    x.append(float(line.split()[0]))\n",
    "                    y.append(float(line.split()[1]))\n",
    "                y = [yi-min(y) for yi in y]\n",
    "                x, y = smooth_spectrum(x, y)\n",
    "                fl_int_norm = [yi/max(y) for yi in y]\n",
    "                fl_auc_total = get_area_under_peaks(x, y)\n",
    "                peaks, areas = process_spectrum(x, y, norm_area=fl_auc_total)\n",
    "                fl_wvl, fl_int = x[:], y[:]\n",
    "    \n",
    "    #if os.path.exists(f\"Data/fluorescence_data/proposed_trials/NLP/{csv}\"):\n",
    "    if os.path.exists(f\"Data/fluorescence_data/proposed_trials/BO/{csv}\"):\n",
    "                df = pd.read_csv(f\"Data/fluorescence_data/{csv}\")\n",
    "                df.sort_values(by=['nm'], inplace=True) # wavelengths were saved in reversed order\n",
    "                x, y = df['nm'].astype(float).to_list(), df[' A'].astype(float).to_list()\n",
    "                x, y = smooth_spectrum(x, y)\n",
    "                abs_int_norm = [yi/max(y) for yi in y]\n",
    "                abs_auc_total = get_area_under_peaks(x, y)\n",
    "                abs_wvl, abs_int = x[:], y[:]\n",
    "                # qy = qy_r * (fl_s/fl_r) * (abs_r/abs_s) * (n_s/n_r)**2 Constant values took from Farwa's spreadsheet\n",
    "                abs_s = df[df['nm'] == 430][' A'].values[0]\n",
    "                fl_r, abs_r, n_r = 8272505, 0.129734, 1.3611\n",
    "                fl_s, abs_s, n_s = fl_auc_total, abs_s, 1.375\n",
    "                qy = 0.53 * (fl_auc_total/fl_r) * (abs_r/abs_s) * (n_s/n_r)**2\n",
    "                print(abs_s,\"absorbance at 430\",csv)\n",
    "\n",
    "    print(txt,\"QY:\",qy,\"AUC2:\", areas[1],\"f:\", qy*areas[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_(d):\n",
    "    if d.is_integer():\n",
    "        return f\"{int(d)}\"\n",
    "    else:\n",
    "        decimal = len(str(d).split('.')[1])\n",
    "        s = f\"{d:.{decimal}f}\"\n",
    "        return s\n",
    "experiments = pd.read_csv('Data/experiments.csv')   # Total of 56 experiments of raw data\n",
    "                                                    # some data is incomplete, and depending on the purpose of the analysis we might want to discard some of it (e.g.\n",
    "                                                    # we dont have pre-doping absorbance so we cant calculate predoping QY (pre-QY) for some of the NPLs)\n",
    "processed_df = pd.DataFrame()\n",
    "for exp in experiments['Experiment'].unique():\n",
    "    exp_df = experiments[experiments['Experiment'] == exp]\n",
    "\n",
    "    for i, setup in exp_df[['Temperature(C)', 'Doping(%)', 'Time(min)']].iterrows():\n",
    "        T, d, t = setup\n",
    "        with open(f\"Data/fluorescence_data/{exp}/{int(T)}-{f_(d)}-{int(t)}.txt\", 'r') as input:\n",
    "            lines = input.readlines()\n",
    "            x = []\n",
    "            y = []\n",
    "            for line in lines:\n",
    "                x.append(float(line.split()[0]))\n",
    "                y.append(float(line.split()[1]))\n",
    "            x, y = smooth_spectrum(x, y)\n",
    "            #substract minimum value to all values\n",
    "            y = [yi-min(y) for yi in y]\n",
    "            fl_int_norm = [yi/max(y) for yi in y]\n",
    "            fl_auc_total = get_area_under_peaks(x, y)\n",
    "            peaks, areas = process_spectrum(x, y, norm_area=fl_auc_total)\n",
    "            fl_wvl, fl_int = x[:], y[:]\n",
    "\n",
    "        if os.path.exists(f\"Data/absorbance_data/{exp}/{int(T)}-{f_(d)}-{int(t)}.csv\"):\n",
    "            df = pd.read_csv(f\"Data/absorbance_data/{exp}/{int(T)}-{f_(d)}-{int(t)}.csv\")\n",
    "            df.sort_values(by=['nm'], inplace=True) # wavelengths were saved in reversed order\n",
    "            x, y = df['nm'].astype(float).to_list(), df[' A'].astype(float).to_list()\n",
    "            x, y = smooth_spectrum(x, y)\n",
    "            abs_int_norm = [yi/max(y) for yi in y]\n",
    "            abs_auc_total = get_area_under_peaks(x, y)\n",
    "            abs_wvl, abs_int = x[:], y[:]\n",
    "            # qy = qy_r * (fl_s/fl_r) * (abs_r/abs_s) * (n_s/n_r)**2 Constant values took from Farwa's spreadsheet\n",
    "            abs_s = df[df['nm'] == 430][' A'].values[0]\n",
    "            fl_r, abs_r, n_r = 8272505, 0.129734, 1.3611\n",
    "            fl_s, abs_s, n_s = fl_auc_total, abs_s, 1.375\n",
    "            qy = 0.53 * (fl_auc_total/fl_r) * (abs_r/abs_s) * (n_s/n_r)**2\n",
    "            qypeak1 = 0.53 * (areas[0]*fl_auc_total/fl_r) * (abs_r/abs_s) * (n_s/n_r)**2\n",
    "            qypeak2 = 0.53 * (areas[1]*fl_auc_total/fl_r) * (abs_r/abs_s) * (n_s/n_r)**2\n",
    "        else:\n",
    "            print(f\"File not available: Data/absorbance_data/{exp}/{int(T)}-{f_(d)}-{int(t)}.csv\")\n",
    "            abs_int_norm = None\n",
    "            abs_auc_total = None\n",
    "            abs_wvl, abs_int = None, None\n",
    "            qy = -1\n",
    "        #if \"pre doping\" in folder \n",
    "        \"\"\"Some of the NPLs where measured after synthesis/before doping \"\"\"\n",
    "        pre_fl_files = [\"NPL-4-Predoping 100x dilution 3mL cuvette.txt\",\n",
    "                 \"NPL-11 Pre Doping Stock-100x dilution (3mL).txt\",\n",
    "                 \"NPL-17 100x Pre Doping.txt\",\n",
    "                 \"NPL-17 100x Pre doping stock.txt\",\n",
    "                 \"NPL-12 Pre Doping 100x dil.txt\",\n",
    "                 \"NPL-13 Pre Doping 100x dil.txt\"]\n",
    "        pre_abs_files = [\"NPL-4 100x Pre Doping .csv\",\n",
    "                         \"NPL-11 Pre Doping Stock-100x dilution (3mL).csv\",\n",
    "                         \"NPL-17 100x Pre Doping.csv\",\n",
    "                         \"NPL-17 100x Pre doping stock.csv\",\n",
    "                         \"NPL-12 1x Pre doping 100 x.Sample.Raw.csv\",\n",
    "                         \"NPL-13 Pre doping 100 x dil .Sample.Raw.csv\"]\n",
    "        #iterate files in directory.\n",
    "        for file in os.listdir(f\"Data/fluorescence_data/{exp}\"):\n",
    "            if file in pre_fl_files:\n",
    "                print(file)\n",
    "                with open(f\"Data/fluorescence_data/{exp}/{file}\", 'r') as input:\n",
    "                    lines = input.readlines()\n",
    "                    x = []\n",
    "                    y = []\n",
    "                    for line in lines:\n",
    "                        x.append(float(line.split()[0]))\n",
    "                        y.append(float(line.split()[1]))\n",
    "                    x, y = smooth_spectrum(x, y)\n",
    "\n",
    "                    y = [yi-min(y) for yi in y]                      #substract minimum value to all values (baseline correction)\n",
    "                    pre_fl_int_norm = [yi/max(y) for yi in y]\n",
    "                    pre_fl_auc_total = get_area_under_peaks(x, y)\n",
    "                    pre_peaks, pre_areas = process_spectrum(x, y, norm_area=fl_auc_total)\n",
    "                    pre_fl_wvl, pre_fl_int = x[:], y[:]\n",
    "        flag = 0\n",
    "        for file in os.listdir(f\"Data/absorbance_data/{exp}\"):\n",
    "            if file in pre_abs_files:\n",
    "                flag = 1\n",
    "                df = pd.read_csv(f\"Data/absorbance_data/{exp}/{file}\")\n",
    "                df.sort_values(by=['nm'], inplace=True) # wavelengths were saved in reversed order\n",
    "                x, y = df['nm'].astype(float).to_list(), df[' A'].astype(float).to_list()\n",
    "                x, y = smooth_spectrum(x, y)\n",
    "                abs_int_norm = [yi/max(y) for yi in y]\n",
    "                abs_auc_total = get_area_under_peaks(x, y)\n",
    "                abs_wvl, abs_int = x[:], y[:]\n",
    "                # qy = qy_r * (fl_s/fl_r) * (abs_r/abs_s) * (n_s/n_r)**2 Constant values took from Farwa's spreadsheet\n",
    "                pre_abs_s = df[df['nm'] == 430][' A'].values[0]\n",
    "                pre_fl_r, pre_abs_r, n_r = 8272505, 0.129734, 1.3611\n",
    "                pre_fl_s, pre_abs_s, n_s = pre_fl_auc_total, pre_abs_s, 1.375\n",
    "                print(pre_fl_auc_total,pre_fl_r,pre_abs_r,pre_abs_s)\n",
    "                pre_qy = 0.53 * (pre_fl_auc_total/pre_fl_r) * (pre_abs_r/pre_abs_s) * (n_s/n_r)**2\n",
    "        if flag == 0:\n",
    "                print(f\"File not available: Data/absorbance_data/{exp}\")\n",
    "                abs_int_norm = None\n",
    "                abs_auc_total = None\n",
    "                abs_wvl, abs_int = None, None\n",
    "                pre_qy = -1  #if we dont have pre-doping absorbance we cant calculate pre-doping QY (-1 is the flag to not use the data)\n",
    "             \n",
    "        #we add whatever we want to the results_df, we can add other labels here just be sure to calculate them\n",
    "        #above and any new util function should be added to the top of the file\n",
    "        results_df = pd.DataFrame({'Temperature(C)': [T], \n",
    "                                    'Doping(%)': [d], \n",
    "                                    'Time(min)': [t], \n",
    "                                    'Peak1': [peaks[0]], \n",
    "                                    'Peak2': [peaks[1]], \n",
    "                                    'Area1': [areas[0]], \n",
    "                                    'Area2': [areas[1]],\n",
    "                                    'Pre-QY': [pre_qy],\n",
    "                                    'QY': [qy],\n",
    "                                    'QY_peak1': [qypeak1],\n",
    "                                    'QY_peak2': [qypeak2],\n",
    "                                    # 'Distance_peaks': [peaks[1]-peaks[0]],\n",
    "                                    # 'ratioP2P1': [areas[1]/areas[0]],\n",
    "                                    'Fl_Wavelengths': [fl_wvl],\n",
    "                                    'Fluorescence': [fl_int],\n",
    "                                    'Fluorescence_norm': [fl_int_norm],\n",
    "                                    'Abs_Wavelengths': [abs_wvl],\n",
    "                                    'Absorbance': [abs_int],\n",
    "                                    'Absorbance_norm': [abs_int_norm]\n",
    "                                    }) # we can add other labels here\n",
    "        new_entry = exp_df.merge(results_df, how='inner', on=['Temperature(C)', 'Doping(%)', 'Time(min)'])\n",
    "        processed_df = pd.concat([processed_df, new_entry], ignore_index=True)\n",
    "processed_df.to_csv('Data/processed_datawobaselinev4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making data for Bayesian Optimization with GPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/processed_datawobaselinev4.csv')\n",
    "\n",
    "\n",
    "#Maintain only Dimension1(nm),Dimension2(nm),OD,Dilution(%),Nano_stock_vol(mL),Diluted_vol(mL),AgConc(M),AgMass(mg),MeOH_vol(mL),H2O_vol(mL),Doping(%),AgSol(uL),Time(min),Temperature(C),Experiment,\n",
    "grouped = df.groupby(['Experiment','Doping(%)', 'Time(min)', 'Temperature(C)'])\n",
    "features_labels_dict = {}\n",
    "\n",
    "for name, group in grouped:\n",
    "    features = group[['Dimension1(nm)', 'Dimension2(nm)', 'Dilution(%)', 'Nano_stock_vol(mL)', \n",
    "                      'Diluted_vol(mL)', 'AgConc(M)', 'AgMass(mg)', 'MeOH_vol(mL)', \n",
    "                      'H2O_vol(mL)', 'Doping(%)', 'AgSol(uL)', 'Time(min)', 'Temperature(C)']] #we can add more features here or remove some of them\n",
    "    \n",
    "    label = group['Area2']\n",
    "    label2 = group['QY']\n",
    "    \n",
    "    features_labels_dict[name] = {'features': features, 'label': label, 'labelqy': label2}\n",
    "group_keys = list(features_labels_dict.keys())\n",
    "#remove points where qy == -1\n",
    "\n",
    "predictions = {}\n",
    "# Get the group keys for training and testing\n",
    "train_keys = [group_keys[i] for i in range(len(group_keys)) ]\n",
    "test_keys = group_keys  # test_keys_index is a list with one elemen\n",
    "# Prepare the training and testing data\n",
    "train_features = np.vstack([features_labels_dict[key]['features'] for key in train_keys])\n",
    "train_labels = np.hstack([features_labels_dict[key]['label'] for key in train_keys])\n",
    "train_labels2 = np.hstack([features_labels_dict[key]['labelqy'] for key in train_keys])\n",
    "#remove data with -1 in QY for both qy and area2/ -1 is our flag for missing data\n",
    "train_labels = train_labels[train_labels2 != -1]  \n",
    "train_features = train_features[train_labels2 != -1]\n",
    "train_labels2 = train_labels2[train_labels2 != -1]\n",
    "if len(train_labels) == len(train_labels2)== len(train_features):\n",
    "    train_final_label = train_labels * train_labels2\n",
    "else:\n",
    "     print(\"this is not working\")\n",
    "\n",
    "\n",
    "modified_features = train_features.copy()\n",
    "#only use the independent features\n",
    "modified_features = modified_features[:,[0,1,2,3,5,6,9,11,12]] #TODO: Make it such that i can use the names for clarity.\n",
    "\n",
    "#convert to tensor\n",
    "#modified_features = torch.tensor(modified_features.values, dtype=torch.float)\n",
    "#train_final_label = torch.tensor(train_final_label.values, dtype=torch.float)\n",
    "#make sure to round up value up to two decimals\n",
    "#save modified features and train_final_label to csv\n",
    "df = pd.DataFrame(modified_features)\n",
    "df['label'] = train_labels2\n",
    "NAME_OF_FILE = 'Data/modified_featuresv4.csv'\n",
    "df.to_csv(NAME_OF_FILE)\n",
    "with open(f'{NAME_OF_FILE}_description.txt', 'a') as f:\n",
    "    \"\"\"Describe the columns of the modified features csv\n",
    "    and what are you changing or wanting to analyze from this dataset\"\"\"\n",
    "    description = \"\"\"###ENTER DESCRIPTION HERE###\n",
    "    ###Example: Dataset where i include pre-qy to the features. \n",
    "    ###Example: My hypothesis is that it will improve the model performance.\"\"\"\n",
    "    f.write(f\"Final_label,{train_final_label}\\n\")\n",
    "    f.write(description)\n",
    "\n",
    "#analysis of the data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
