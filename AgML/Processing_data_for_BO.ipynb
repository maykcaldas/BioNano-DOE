{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as font_manager\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://github.com/google/fonts/raw/main/ofl/ibmplexmono/IBMPlexMono-Regular.ttf\",\n",
    "    \"IBMPlexMono-Regular.ttf\",\n",
    ")\n",
    "fe = font_manager.FontEntry(fname=\"IBMPlexMono-Regular.ttf\", name=\"plexmono\")\n",
    "font_manager.fontManager.ttflist.append(fe)\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"axes.facecolor\": \"#f5f4e9\",\n",
    "        \"grid.color\": \"#AAAAAA\",\n",
    "        \"axes.edgecolor\": \"#333333\",\n",
    "        \"figure.facecolor\": \"#FFFFFF\",\n",
    "        \"axes.grid\": False,\n",
    "        \"axes.prop_cycle\": plt.cycler(\"color\", plt.cm.Dark2.colors),\n",
    "        \"font.family\": fe.name,\n",
    "        \"figure.figsize\": (3.5, 3.5 / 1.2),\n",
    "        \"ytick.left\": True,\n",
    "        \"xtick.bottom\": True,\n",
    "    }\n",
    ")\n",
    "\n",
    "import random\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat('NormFLEff.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(mat['FLEff'][:,0],mat['FLEff'][:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the detector PL efficiency curve\n",
    "# FLEff = np.load('NormFLEff.npy')  # Assuming you saved 'NormFLEff.mat' as a .npy file\n",
    "FLEff = mat\n",
    "# Directory listing and file range\n",
    "files = sorted(os.listdir('/Users/jorgemedina/DOEFinal/DOE/AgML/Data/proposed_trials/BO'))\n",
    "# inS = 5  # start index of files\n",
    "# inE = 13  # end index of files\n",
    "files = [file for file in files if \".txt\" in file]\n",
    "\n",
    "# Gaussian peak selection\n",
    "GPk = np.ones(len(files))  # Set to 1 for single peak, modify as needed\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLEff = FLEff['FLEff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "dirs= [\"/Users/jorgemedina/DOEFinal/DOE/AgML/Data/proposed_trials/NLP2\"]\n",
    "\n",
    "# Loop over each file\n",
    "for directory in dirs:\n",
    "    # Get the list of files\n",
    "    files = sorted(os.listdir(directory))\n",
    "    files = [file for file in files if (\".txt\" in file and \"corrected\" not in file)]\n",
    "    print(files)\n",
    "    GPk = np.ones(len(files))  # Set to 1 for single peak, modify as needed\n",
    "    for ind in range(len(files)):\n",
    "        # Clear variables\n",
    "        EffS = EffE = Eff = S = None\n",
    "        \n",
    "        # Read the data\n",
    "        # path = '/Users/jorgemedina/DOEFinal/DOE/AgML/Data/proposed_trials/BO/'\n",
    "        path = directory + '/'\n",
    "        tmp = pd.read_csv(path+files[ind], sep='\\t').to_numpy()  # Adjust delimiter if needed\n",
    "        WLS = tmp[:, 0]  # Wavelength data\n",
    "        bkg = np.mean(tmp[:35, 1])  # Background calculation\n",
    "        print(bkg)\n",
    "        S = tmp[:, 1] - bkg  # Subtract background from signal\n",
    "        # Apply efficiency curve\n",
    "        print(tmp[-1,0])\n",
    "        EffS = np.where(FLEff[:, 0] == tmp[0, 0])[0][0]  # Start of Eff curve\n",
    "        EffE = np.where(FLEff[:, 0] == tmp[-1, 0])[0][0]  # End of Eff curve\n",
    "        Eff = FLEff[EffS:EffE+1, 1]\n",
    "        WLEff = FLEff[EffS:EffE+1, 0]  # Corrected wavelength array\n",
    "        print(len(S))\n",
    "        print(len(Eff))\n",
    "        tmpC = S / Eff  # Apply efficiency correction\n",
    "        # Fit range selection\n",
    "        fitS = 441  # Start of fit range\n",
    "        fitE = 800  # End of fit range\n",
    "        i1 = np.where(WLS == fitS)[0][0]\n",
    "        i2 = np.where(WLS == fitE)[0][0]\n",
    "        print(i1,i2)\n",
    "        tmpCF = tmpC[i1:i2]\n",
    "        tmpWLF = WLS[i1:i2]\n",
    "    \n",
    "        # Gaussian fitting function\n",
    "        def gauss1(x, a,b,c):\n",
    "            return a/((2 * np.pi *c )**0.5) * np.exp(-((x - b) ** 2) / (2 * c ))\n",
    "    \n",
    "        def gauss2(x, a1, b1, c1, a2, b2, c2):\n",
    "            return (a1 * np.exp(-((x - b1) ** 2) / (2 * c1 ** 2)) +\n",
    "                    a2 * np.exp(-((x - b2) ** 2) / (2 * c2 ** 2)))\n",
    "    \n",
    "        # Perform the Gaussian fit\n",
    "        plt.plot(tmpWLF, tmpCF, label='To Fit Range')\n",
    "        plt.show()\n",
    "        flag = 0\n",
    "        if GPk[ind] != 1:\n",
    "            print('here')\n",
    "            popt, _ = curve_fit(gauss1, tmpWLF, tmpCF, p0=[10000,680,3000])\n",
    "            yFit = gauss1(np.arange(fitS, 1001), *popt)\n",
    "        else:\n",
    "            popt, _ = curve_fit(gauss2, tmpWLF, tmpCF,p0=[10000,511,1.5,100000,680,100])\n",
    "            if popt[3] < 0:\n",
    "                print('here at index:', ind)\n",
    "                popt, _ = curve_fit(gauss1, tmpWLF, tmpCF, p0=[10000,511,1.5])\n",
    "                flag = 1\n",
    "        if flag == 1:\n",
    "            yFit = gauss1(np.arange(fitS, 1001), *popt)\n",
    "        else:\n",
    "            yFit = gauss2(np.arange(fitS, 1001), *popt)\n",
    "        # print(popt)\n",
    "        # Plot corrected data and fit\n",
    "        if flag == 1:\n",
    "            plt.figure(1)\n",
    "            plt.clf()\n",
    "            plt.plot(WLS, tmpC, label='Corrected')\n",
    "            plt.plot(tmpWLF, tmpCF, label='Fit Range')\n",
    "            plt.plot(np.arange(fitS, 1001), yFit, label='Gaussian Fit')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "        # Combine the corrected spectra\n",
    "        WLFinal = np.arange(WLS[0], 1001)\n",
    "        SFinal = tmpC\n",
    "        #save the two columns WLC and yFit to a file\n",
    "        name = files[ind].split('.')[:-1]\n",
    "        name = '.'.join(name)\n",
    "        np.savetxt(path+name+'_corrected.txt', np.column_stack((WLFinal, yFit)), delimiter='\\t')\n",
    "        #save the data to a file with the suffix _corrected\n",
    "        if flag == 1:\n",
    "            np.save(path+files[ind].split('.')[0]+'_corrected', SFinal[i1:len(WLFinal)])\n",
    "            flag = 0\n",
    "        # SFinal[i1:len(WLFinal)] = yFit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the corrected data and the non correcte data \n",
    "directory = '/Users/jorgemedina/DOEFinal/DOE/AgML/Data/proposed_trials/NLP2/'\n",
    "files = sorted(os.listdir(directory))\n",
    "files = [file for file in files if \".txt\" in file]\n",
    "files = [file for file in files if \"_corrected\" not in file]\n",
    "for file in files:\n",
    "    tmp = pd.read_csv(path+file, sep='\\t').to_numpy()\n",
    "    try:\n",
    "        tmpC = pd.read_csv(path+file.split('.')[0]+'_corrected.txt', sep='\\t').to_numpy()\n",
    "    except:\n",
    "        tmpC = pd.read_csv(path+file, sep='\\t').to_numpy()\n",
    "    plt.plot(tmp[:,0],tmp[:,1],label='Original')\n",
    "    plt.plot(tmpC[:,0],tmpC[:,1],label='Corrected')\n",
    "    plt.title(file)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "#make the same but for each directory, put them all in the same plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util functions to process Spectre information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "# TODO: Refactor all these functions to take into account that we also have abs spectra now\n",
    "\n",
    "def _get_peaks(x, y):\n",
    "    peaks = []\n",
    "    th = 0.001\n",
    "    for i in range(1, len(x)-1):\n",
    "        if y[i] > y[i-1]+th and y[i] > y[i+1]+th:\n",
    "            peaks.append((x[i], y[i]))\n",
    "    return peaks\n",
    "\n",
    "def get_peaks(x, y):\n",
    "    x = np.array(x)\n",
    "    p1_i = np.where(x == 450)[0][0]\n",
    "    p1_e = np.where(x == 550)[0][0]\n",
    "    p2_i = np.where(x == 550)[0][0]\n",
    "    p2_e = np.where(x == 800)[0][0]\n",
    "\n",
    "    x1 = x[p1_i:p1_e]\n",
    "    y1 = y[p1_i:p1_e]\n",
    "    x2 = x[p2_i:p2_e]\n",
    "    y2 = y[p2_i:p2_e]\n",
    "\n",
    "    return x1[np.argmax(y1)], x2[np.argmax(y2)]\n",
    "\n",
    "def get_area_under_peaks(x, y, norm=None):\n",
    "    if not norm:\n",
    "        return np.trapz(y, x)\n",
    "    \n",
    "    x = np.array(x)\n",
    "    p1_i = np.where(x == 450)[0][0]\n",
    "    p1_e = np.where(x == 550)[0][0]\n",
    "    p2_i = np.where(x == 550)[0][0]\n",
    "    p2_e = np.where(x == 800)[0][0]\n",
    "\n",
    "    x1 = x[p1_i:p1_e]\n",
    "    y1 = y[p1_i:p1_e]\n",
    "    x2 = x[p2_i:p2_e]\n",
    "    y2 = y[p2_i:p2_e]\n",
    "\n",
    "    return np.trapz(y1, x1)/norm, np.trapz(y2, x2)/norm\n",
    "\n",
    "def process_spectrum(x, y, norm_area):\n",
    "    peaks = get_peaks(x, y)\n",
    "    areas = get_area_under_peaks(x, y, norm_area)\n",
    "    return peaks, areas\n",
    "\n",
    "def smooth_spectrum(x, y, window=4):\n",
    "    y = np.array(y)\n",
    "    y_smooth = np.zeros_like(y)\n",
    "    for i in range(len(y)):\n",
    "        if i < window:\n",
    "            y_smooth[i] = np.mean(y[:i+window])\n",
    "        elif i > len(y) - window:\n",
    "            y_smooth[i] = np.mean(y[i-window:])\n",
    "        else:\n",
    "            y_smooth[i] = np.mean(y[i-window:i+window])\n",
    "    return x, y_smooth.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get f=AUC2*QY for new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of files in Data/fluoressence_data starting in M\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "files = os.listdir('Data/proposed_trials/BO')\n",
    "#files = os.listdir('Data/fluorescence_data/proposed_trials/NLP/')\n",
    "#split files in two lists, one .txt and one .csv\n",
    "txt_files = [file for file in files if file.endswith('.txt') and 'corrected' in file]\n",
    "csv_files = [file for file in files if file.endswith('.csv')]\n",
    "#organize them together such that the first .txt file corresponds to the first .csv file\n",
    "txt_files.sort() #To make sure they are zipped in the right order correctly\n",
    "csv_files.sort() #To make sure they are zipped in the right order correctly\n",
    "\n",
    "for i, (txt, csv) in enumerate(zip(txt_files, csv_files)):\n",
    "    #with open(f\"Data/fluorescence_data/proposed_trials/NLP/{txt}\", 'r') as input:\n",
    "    try:\n",
    "        with open(f\"Data/proposed_trials/BO/{txt}\", 'r',encoding='utf-8-sig') as input:\n",
    "                    lines = input.readlines()\n",
    "                    x = []\n",
    "                    y = []\n",
    "                    for line in lines:\n",
    "                        x.append(float(line.split()[0]))\n",
    "                        y.append(float(line.split()[1]))\n",
    "                    y = [yi-min(y) for yi in y]\n",
    "                    x, y = smooth_spectrum(x, y)\n",
    "                    #plot_spectrum(x, y, txt)\n",
    "                    plt.plot(x, y,color='C1')\n",
    "                    plt.xlabel('Wavelength (nm)')\n",
    "                    plt.ylabel('Fluorescense Intensity (counts)')\n",
    "\n",
    "                    fl_int_norm = [yi/max(y) for yi in y]\n",
    "                    fl_auc_total = get_area_under_peaks(x, y)\n",
    "                    peaks, areas = process_spectrum(x, y, norm_area=fl_auc_total)\n",
    "                    fl_wvl, fl_int = x[:], y[:]\n",
    "        \n",
    "        #if os.path.exists(f\"Data/fluorescence_data/proposed_trials/NLP/{csv}\"):\n",
    "        if os.path.exists(f\"Data/proposed_trials/BO/{csv}\"):\n",
    "                    df = pd.read_csv(f\"Data/proposed_trials/BO/{csv}\")\n",
    "                    df.sort_values(by=['nm'], inplace=True) # wavelengths were saved in reversed order\n",
    "                    x, y = df['nm'].astype(float).to_list(), df[' A'].astype(float).to_list()\n",
    "                    x, y = smooth_spectrum(x, y)\n",
    "                    abs_int_norm = [yi/max(y) for yi in y]\n",
    "                    abs_auc_total = get_area_under_peaks(x, y)\n",
    "                    abs_wvl, abs_int = x[:], y[:]\n",
    "                    # qy = qy_r * (fl_s/fl_r) * (abs_r/abs_s) * (n_s/n_r)**2 Constant values took from Farwa's spreadsheet\n",
    "                    abs_s = df[df['nm'] == 430][' A'].values[0]\n",
    "                    fl_r, abs_r, n_r = 8272505, 0.129734, 1.3611\n",
    "                    fl_s, abs_s, n_s = fl_auc_total, abs_s, 1.375\n",
    "                    qy = 0.53 * (fl_auc_total/fl_r) * (abs_r/abs_s) * (n_s/n_r)**2\n",
    "                    #two decimals\n",
    "                    plt.title(f\"Trial: {i+1} \\nQY={qy:.2f}  AUC2= {areas[1]:.2f}  F={qy*areas[1]:.2f}\",fontsize=10)\n",
    "                    plt.show()\n",
    "                    print(abs_s,\"absorbance at 430\",csv)\n",
    "    except Exception as e:\n",
    "           print(f\"Exception at {txt} and {csv}\")\n",
    "           print(e)\n",
    "           continue\n",
    "    print(txt,\"QY:\",qy,\"AUC2:\", areas[1],\"f:\", qy*areas[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "files = os.listdir('Data/proposed_trials/pre-doping/')\n",
    "#files = os.listdir('Data/fluorescence_data/proposed_trials/NLP/')\n",
    "#split files in two lists, one .txt and one .csv\n",
    "txt_files = [file for file in files if file.endswith('.txt')]\n",
    "csv_files = [file for file in files if file.endswith('.csv')]\n",
    "#organize them together such that the first .txt file corresponds to the first .csv file\n",
    "txt_files.sort() #To make sure they are zipped in the right order correctly\n",
    "csv_files.sort() #To make sure they are zipped in the right order correctly\n",
    "for txt,csv in zip(txt_files,csv_files):\n",
    "    print(txt,csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of files in Data/fluoressence_data starting in M\n",
    "\n",
    "\n",
    "for i, (txt, csv) in enumerate(zip(txt_files, csv_files)):\n",
    "    #with open(f\"Data/fluorescence_data/proposed_trials/NLP/{txt}\", 'r') as input:\n",
    "    try:\n",
    "        with open(f\"Data/proposed_trials/pre-doping/{txt}\", 'r',encoding='utf-8-sig') as input:\n",
    "                    lines = input.readlines()\n",
    "                    x = []\n",
    "                    y = []\n",
    "                    for line in lines:\n",
    "                        x.append(float(line.split()[0]))\n",
    "                        y.append(float(line.split()[1]))\n",
    "                    y = [yi-min(y) for yi in y]\n",
    "                    x, y = smooth_spectrum(x, y)\n",
    "                    #plot_spectrum(x, y, txt)\n",
    "                    plt.plot(x, y,color='C1')\n",
    "                    plt.xlabel('Wavelength (nm)')\n",
    "                    plt.ylabel('Fluorescense Intensity (counts)')\n",
    "\n",
    "                    fl_int_norm = [yi/max(y) for yi in y]\n",
    "                    fl_auc_total = get_area_under_peaks(x, y)\n",
    "                    peaks, areas = process_spectrum(x, y, norm_area=fl_auc_total)\n",
    "                    fl_wvl, fl_int = x[:], y[:]\n",
    "        \n",
    "        #if os.path.exists(f\"Data/fluorescence_data/proposed_trials/NLP/{csv}\"):\n",
    "        if os.path.exists(f\"Data/proposed_trials/pre-doping/{csv}\"):\n",
    "                    df = pd.read_csv(f\"Data/proposed_trials/pre-doping/{csv}\")\n",
    "                    df.sort_values(by=['nm'], inplace=True) # wavelengths were saved in reversed order\n",
    "                    x, y = df['nm'].astype(float).to_list(), df[' A'].astype(float).to_list()\n",
    "                    x, y = smooth_spectrum(x, y)\n",
    "                    abs_int_norm = [yi/max(y) for yi in y]\n",
    "                    abs_auc_total = get_area_under_peaks(x, y)\n",
    "                    abs_wvl, abs_int = x[:], y[:]\n",
    "                    # qy = qy_r * (fl_s/fl_r) * (abs_r/abs_s) * (n_s/n_r)**2 Constant values took from Farwa's spreadsheet\n",
    "                    abs_s = df[df['nm'] == 430][' A'].values[0]\n",
    "                    fl_r, abs_r, n_r = 8272505, 0.129734, 1.3611\n",
    "                    fl_s, abs_s, n_s = fl_auc_total, abs_s, 1.375\n",
    "                    qy = 0.53 * (fl_auc_total/fl_r) * (abs_r/abs_s) * (n_s/n_r)**2\n",
    "                    #two decimals\n",
    "                    plt.title(f\"Trial: {i+1}| {txt} \\nQY={qy:.2f}  AUC2= {areas[1]:.2f}  F={qy*areas[1]:.2f}\",fontsize=10)\n",
    "                    plt.show()\n",
    "                    print(abs_s,\"absorbance at 430\",csv)\n",
    "    except Exception as e:\n",
    "           print(f\"Exception at {txt} and {csv}\")\n",
    "           print(e)\n",
    "           continue\n",
    "    print(txt,\"QY:\",qy,\"AUC2:\", areas[1],\"f:\", qy*areas[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the corrected data and the non correcte data on all dirs\n",
    "for directory in dirs:\n",
    "    files = sorted(os.listdir(directory))\n",
    "    for file in files:\n",
    "        tmp = pd.read_csv(directory+'/'+file, sep='\\t').to_numpy()\n",
    "        try:\n",
    "            tmpC = pd.read_csv(directory+'/'+file.split('.')[0]+'_corrected.txt', sep='\\t').to_numpy()\n",
    "        except:\n",
    "            tmpC = pd.read_csv(directory+'/'+file, sep='\\t').to_numpy()\n",
    "        plt.plot(tmp[:,0],tmp[:,1],label='Original')\n",
    "        plt.plot(tmpC[:,0],tmpC[:,1],label='Corrected')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_(d):\n",
    "    if d.is_integer():\n",
    "        return f\"{int(d)}\"\n",
    "    else:\n",
    "        decimal = len(str(d).split('.')[1])\n",
    "        s = f\"{d:.{decimal}f}\"\n",
    "        return s\n",
    "experiments = pd.read_csv('Data/experiments.csv')   # Total of 56 experiments of raw data\n",
    "                                                    # some data is incomplete, and depending on the purpose of the analysis we might want to discard some of it (e.g.\n",
    "                                                    # we dont have pre-doping absorbance so we cant calculate predoping QY (pre-QY) for some of the NPLs)\n",
    "processed_df = pd.DataFrame()\n",
    "for exp in experiments['Experiment'].unique():\n",
    "    exp_df = experiments[experiments['Experiment'] == exp]\n",
    "\n",
    "    for i, setup in exp_df[['Temperature(C)', 'Doping(%)', 'Time(min)']].iterrows():\n",
    "        T, d, t = setup\n",
    "        with open(f\"Data/fluorescence_data/{exp}/{int(T)}-{f_(d)}-{int(t)}_corrected.txt\", 'r') as input:\n",
    "            lines = input.readlines()\n",
    "            x = []\n",
    "            y = []\n",
    "            for line in lines:\n",
    "                x.append(float(line.split()[0]))\n",
    "                y.append(float(line.split()[1]))\n",
    "            x, y = smooth_spectrum(x, y)\n",
    "            #substract minimum value to all values\n",
    "            y = [yi-min(y) for yi in y]\n",
    "            fl_int_norm = [yi/max(y) for yi in y]\n",
    "            fl_auc_total = get_area_under_peaks(x, y)\n",
    "            peaks, areas = process_spectrum(x, y, norm_area=fl_auc_total)\n",
    "            fl_wvl, fl_int = x[:], y[:]\n",
    "\n",
    "        if os.path.exists(f\"Data/absorbance_data/{exp}/{int(T)}-{f_(d)}-{int(t)}.csv\"):\n",
    "            df = pd.read_csv(f\"Data/absorbance_data/{exp}/{int(T)}-{f_(d)}-{int(t)}.csv\")\n",
    "            df.sort_values(by=['nm'], inplace=True) # wavelengths were saved in reversed order\n",
    "            x, y = df['nm'].astype(float).to_list(), df[' A'].astype(float).to_list()\n",
    "            x, y = smooth_spectrum(x, y)\n",
    "            abs_int_norm = [yi/max(y) for yi in y]\n",
    "            abs_auc_total = get_area_under_peaks(x, y)\n",
    "            abs_wvl, abs_int = x[:], y[:]\n",
    "            # qy = qy_r * (fl_s/fl_r) * (abs_r/abs_s) * (n_s/n_r)**2 Constant values took from Farwa's spreadsheet\n",
    "            abs_s = df[df['nm'] == 430][' A'].values[0]\n",
    "            fl_r, abs_r, n_r = 8272505, 0.129734, 1.3611\n",
    "            fl_s, abs_s, n_s = fl_auc_total, abs_s, 1.375\n",
    "            qy = 0.53 * (fl_auc_total/fl_r) * (abs_r/abs_s) * (n_s/n_r)**2\n",
    "            qypeak1 = 0.53 * (areas[0]*fl_auc_total/fl_r) * (abs_r/abs_s) * (n_s/n_r)**2\n",
    "            qypeak2 = 0.53 * (areas[1]*fl_auc_total/fl_r) * (abs_r/abs_s) * (n_s/n_r)**2\n",
    "        else:\n",
    "            print(f\"File not available: Data/absorbance_data/{exp}/{int(T)}-{f_(d)}-{int(t)}.csv\")\n",
    "            abs_int_norm = None\n",
    "            abs_auc_total = None\n",
    "            abs_wvl, abs_int = None, None\n",
    "            qy = -1\n",
    "        #if \"pre doping\" in folder \n",
    "        \"\"\"Some of the NPLs where measured after synthesis/before doping \"\"\"\n",
    "        pre_fl_files = [\"NPL-4-Predoping 100x dilution 3mL cuvette.txt\",\n",
    "                 \"NPL-11 Pre Doping Stock-100x dilution (3mL).txt\",\n",
    "                 \"NPL-17 100x Pre Doping.txt\",\n",
    "                 \"NPL-17 100x Pre doping stock.txt\",\n",
    "                 \"NPL-12 Pre Doping 100x dil.txt\",\n",
    "                 \"NPL-13 Pre Doping 100x dil.txt\"]\n",
    "        pre_abs_files = [\"NPL-4 100x Pre Doping .csv\",\n",
    "                         \"NPL-11 Pre Doping Stock-100x dilution (3mL).csv\",\n",
    "                         \"NPL-17 100x Pre Doping.csv\",\n",
    "                         \"NPL-17 100x Pre doping stock.csv\",\n",
    "                         \"NPL-12 1x Pre doping 100 x.Sample.Raw.csv\",\n",
    "                         \"NPL-13 Pre doping 100 x dil .Sample.Raw.csv\"]\n",
    "        #iterate files in directory.\n",
    "        for file in os.listdir(f\"Data/fluorescence_data/{exp}\"):\n",
    "            if file in pre_fl_files:\n",
    "                print(file)\n",
    "                with open(f\"Data/fluorescence_data/{exp}/{file}\", 'r') as input:\n",
    "                    lines = input.readlines()\n",
    "                    x = []\n",
    "                    y = []\n",
    "                    for line in lines:\n",
    "                        x.append(float(line.split()[0]))\n",
    "                        y.append(float(line.split()[1]))\n",
    "                    x, y = smooth_spectrum(x, y)\n",
    "\n",
    "                    y = [yi-min(y) for yi in y]                      #substract minimum value to all values (baseline correction)\n",
    "                    pre_fl_int_norm = [yi/max(y) for yi in y]\n",
    "                    pre_fl_auc_total = get_area_under_peaks(x, y)\n",
    "                    pre_peaks, pre_areas = process_spectrum(x, y, norm_area=fl_auc_total)\n",
    "                    pre_fl_wvl, pre_fl_int = x[:], y[:]\n",
    "        flag = 0\n",
    "        for file in os.listdir(f\"Data/absorbance_data/{exp}\"):\n",
    "            if file in pre_abs_files:\n",
    "                flag = 1\n",
    "                df = pd.read_csv(f\"Data/absorbance_data/{exp}/{file}\")\n",
    "                df.sort_values(by=['nm'], inplace=True) # wavelengths were saved in reversed order\n",
    "                x, y = df['nm'].astype(float).to_list(), df[' A'].astype(float).to_list()\n",
    "                x, y = smooth_spectrum(x, y)\n",
    "                abs_int_norm = [yi/max(y) for yi in y]\n",
    "                abs_auc_total = get_area_under_peaks(x, y)\n",
    "                abs_wvl, abs_int = x[:], y[:]\n",
    "                # qy = qy_r * (fl_s/fl_r) * (abs_r/abs_s) * (n_s/n_r)**2 Constant values took from Farwa's spreadsheet\n",
    "                pre_abs_s = df[df['nm'] == 430][' A'].values[0]\n",
    "                pre_fl_r, pre_abs_r, n_r = 8272505, 0.129734, 1.3611\n",
    "                pre_fl_s, pre_abs_s, n_s = pre_fl_auc_total, pre_abs_s, 1.375\n",
    "                print(pre_fl_auc_total,pre_fl_r,pre_abs_r,pre_abs_s)\n",
    "                pre_qy = 0.53 * (pre_fl_auc_total/pre_fl_r) * (pre_abs_r/pre_abs_s) * (n_s/n_r)**2\n",
    "        if flag == 0:\n",
    "                print(f\"File not available: Data/absorbance_data/{exp}\")\n",
    "                abs_int_norm = None\n",
    "                abs_auc_total = None\n",
    "                abs_wvl, abs_int = None, None\n",
    "                pre_qy = -1  #if we dont have pre-doping absorbance we cant calculate pre-doping QY (-1 is the flag to not use the data)\n",
    "             \n",
    "        #we add whatever we want to the results_df, we can add other labels here just be sure to calculate them\n",
    "        #above and any new util function should be added to the top of the file\n",
    "        results_df = pd.DataFrame({'Temperature(C)': [T], \n",
    "                                    'Doping(%)': [d], \n",
    "                                    'Time(min)': [t], \n",
    "                                    'Peak1': [peaks[0]], \n",
    "                                    'Peak2': [peaks[1]], \n",
    "                                    'Area1': [areas[0]], \n",
    "                                    'Area2': [areas[1]],\n",
    "                                    'Pre-QY': [pre_qy],\n",
    "                                    'QY': [qy],\n",
    "                                    'QY_peak1': [qypeak1],\n",
    "                                    'QY_peak2': [qypeak2],\n",
    "                                    # 'Distance_peaks': [peaks[1]-peaks[0]],\n",
    "                                    # 'ratioP2P1': [areas[1]/areas[0]],\n",
    "                                    'Fl_Wavelengths': [fl_wvl],\n",
    "                                    'Fluorescence': [fl_int],\n",
    "                                    'Fluorescence_norm': [fl_int_norm],\n",
    "                                    'Abs_Wavelengths': [abs_wvl],\n",
    "                                    'Absorbance': [abs_int],\n",
    "                                    'Absorbance_norm': [abs_int_norm]\n",
    "                                    }) # we can add other labels here\n",
    "        new_entry = exp_df.merge(results_df, how='inner', on=['Temperature(C)', 'Doping(%)', 'Time(min)'])\n",
    "        processed_df = pd.concat([processed_df, new_entry], ignore_index=True)\n",
    "processed_df.to_csv('Data/processed_datawobaselinev7.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making data for Bayesian Optimization with GPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "df = pd.read_csv('Data/processed_datawobaselinev7.csv')\n",
    "\n",
    "\n",
    "#Maintain only Dimension1(nm),Dimension2(nm),OD,Dilution(%),Nano_stock_vol(mL),Diluted_vol(mL),AgConc(M),AgMass(mg),MeOH_vol(mL),H2O_vol(mL),Doping(%),AgSol(uL),Time(min),Temperature(C),Experiment,\n",
    "grouped = df.groupby(['Experiment','Doping(%)', 'Time(min)', 'Temperature(C)'])\n",
    "features_labels_dict = {}\n",
    "\n",
    "for name, group in grouped:\n",
    "    features = group[['Dimension1(nm)', 'Dimension2(nm)', 'Dilution(%)', 'Nano_stock_vol(mL)', \n",
    "                      'Diluted_vol(mL)', 'AgConc(M)', 'AgMass(mg)', 'MeOH_vol(mL)', \n",
    "                      'H2O_vol(mL)', 'Doping(%)', 'AgSol(uL)', 'Time(min)', \n",
    "                      'Temperature(C)','Pre-QY','Fl_Wavelengths','Fluorescence']] #we can add more features here or remove some of them\n",
    "\n",
    "    label = group['Area2']\n",
    "    label2 = group['QY']\n",
    "\n",
    "    features_labels_dict[name] = {'features': features, 'label': label, 'labelqy': label2}\n",
    "group_keys = list(features_labels_dict.keys())\n",
    "\n",
    "predictions = {}\n",
    "# Get the group keys for training and testing\n",
    "train_keys = [group_keys[i] for i in range(len(group_keys)) ]\n",
    "test_keys = group_keys  # test_keys_index is a list with one elemen\n",
    "# Prepare the training and testing data\n",
    "train_features = np.vstack([features_labels_dict[key]['features'] for key in train_keys])\n",
    "train_labels = np.hstack([features_labels_dict[key]['label'] for key in train_keys])\n",
    "train_labels2 = np.hstack([features_labels_dict[key]['labelqy'] for key in train_keys])\n",
    "#remove features whose pre-qy is -1\n",
    "\n",
    "### add cofe here\n",
    "pre_qy_not_minus_one_mask = [features_labels_dict[key]['features']['Pre-QY'] != -1 for key in train_keys]\n",
    "# Apply this mask to train_features, train_labels, and train_labels2\n",
    "train_features = np.vstack([features_labels_dict[key]['features'][pre_qy_not_minus_one_mask[idx]] for idx, key in enumerate(train_keys)])\n",
    "train_labels = np.hstack([features_labels_dict[key]['label'][pre_qy_not_minus_one_mask[idx]] for idx, key in enumerate(train_keys)])\n",
    "train_labels2 = np.hstack([features_labels_dict[key]['labelqy'][pre_qy_not_minus_one_mask[idx]] for idx, key in enumerate(train_keys)])\n",
    "\n",
    "\n",
    "#remove data with -1 in QY for both qy and area2/ -1 is our flag for missing data\n",
    "train_labels = train_labels[train_labels2 != -1]  \n",
    "train_features = train_features[train_labels2 != -1]\n",
    "train_labels2 = train_labels2[train_labels2 != -1]\n",
    "if len(train_labels) == len(train_labels2)== len(train_features):\n",
    "    train_final_label = train_labels * train_labels2\n",
    "else:\n",
    "     print(\"this is not working\")\n",
    "\n",
    "print(train_final_label)\n",
    "modified_features = train_features.copy()\n",
    "print(modified_features[:,-2][1])\n",
    "print(type(modified_features[:,-2][1]))\n",
    "print(modified_features[:,-1][1])\n",
    "print(type(modified_features[:,-1][1]))\n",
    "for i in range(len(modified_features)):\n",
    "    plt.plot(ast.literal_eval(modified_features[:,-2][i]),ast.literal_eval(modified_features[:,-1][i]))\n",
    "    plt.title(f\"Trial: {i+1}\")\n",
    "    print(modified_features[i])\n",
    "    plt.show()\n",
    "#only use the independent features\n",
    "modified_features = modified_features[:,[0,1,2,3,5,6,9,11,12,13]] #TODO: Make it such that i can use the names for clarity.\n",
    "#convert to tensor\n",
    "#modified_features = torch.tensor(modified_features.values, dtype=torch.float)\n",
    "#train_final_label = torch.tensor(train_final_label.values, dtype=torch.float)\n",
    "#make sure to round up value up to two decimals\n",
    "#save modified features and train_final_label to csv\n",
    "df = pd.DataFrame(modified_features)\n",
    "df['label'] = train_final_label\n",
    "NAME_OF_FILE = 'Data/modified_featuresforBOwpreQY_corrected.csv'\n",
    "df.to_csv(NAME_OF_FILE)\n",
    "with open(f'{NAME_OF_FILE}_description_corr.txt', 'a') as f:\n",
    "    \"\"\"Describe the columns of the modified features csv\n",
    "    and what are you changing or wanting to analyze from this dataset\"\"\"\n",
    "    description = \"\"\"###ENTER DESCRIPTION HERE###\n",
    "    This dataset is corrected PL files. This dataset is similar to the original modified_features.csv, but it doesnt include pre-QY.\n",
    "    This is intended for a SR analysis with the pre-QY as an independent variable.\n",
    "    Same as begore, but the label is the normalized AUC2, the product of the area under the second peak and the QY.\n",
    "    \"\"\"\n",
    "    f.write(f\"Final_label,{train_final_label}\\n\")\n",
    "    f.write(description)\n",
    "\n",
    "#analysis of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the data from processed_datawobaselinev6.csv\n",
    "df = pd.read_csv('Data/processed_datawobaselinev6.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data from processed_wobaselinev4.csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "df = pd.read_csv('Data/processed_datawobaselinev4.csv')\n",
    "#group by experiment\n",
    "grouped = list(df.groupby(['Experiment', 'Temperature(C)', 'Doping(%)', 'Time(min)']))\n",
    "#plot the wavelength vs normalized fluorescence intensity for each experiment\n",
    "for name, group, in grouped:\n",
    "\n",
    "    wl = ast.literal_eval(group['Fl_Wavelengths'][0])\n",
    "    fl = ast.literal_eval(group['Fluorescence_norm'][0])\n",
    "    #increase the size\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.plot(wl, fl, label=name)\n",
    "    plt.xlabel('wavelength (nm)')\n",
    "    plt.ylabel('Normalized Fluorescence (a.u.)')\n",
    "    #make the Peak1,Peak2 be vertical lines in the plot\n",
    "    plt.axvline(x=513.1, color='r', linestyle='--', label='Peak1')\n",
    "    plt.axvline(x=626, color='g', linestyle='--', label='Peak2')\n",
    "    plt.show()\n",
    "    break\n",
    "    #for exp, temp,doping,temp in name:\n",
    "    #    plt.plot(group['Fl_Wavelengths'], group['Fluorescence_norm'], label=name)\n",
    "#    plt.plot(group['Fl_Wavelengths'], group['Fluorescence_norm'], label=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DOE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
